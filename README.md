# ğŸ¤Ÿ Sign Language Translator

**Real-time sign language to text and speech translation system**

An ethical, accessibility-first web application that translates sign language gestures into text and speech in real-time. Built with a modern tech stack and designed for production use.

---

## ğŸ¯ Features

âœ… **Real-time Translation** - Instant sign language recognition using webcam  
âœ… **Text-to-Speech** - Automatic speech synthesis with duplicate prevention  
âœ… **Translation History** - Full conversation log with timestamps  
âœ… **Modern UI** - Dark/light themes, responsive design, accessibility-first  
âœ… **Connection Status** - Live indicators for system health  
âœ… **Multi-client Support** - Handle multiple concurrent users  
âœ… **Ethical Design** - Free for individuals with disabilities  

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚  React + TypeScript + Socket.IO
â”‚   (Port 5173)   â”‚  - Webcam capture
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - UI/UX
         â”‚           - Text-to-Speech
         â”‚ WebSocket
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend       â”‚  Node.js + Express + Socket.IO
â”‚   (Port 3000)   â”‚  - WebSocket bridge
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Client management
         â”‚           - Error handling
         â”‚ WebSocket
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML Server     â”‚  Python + FastAPI + PyTorch
â”‚   (Port 8000)   â”‚  - MediaPipe hand tracking
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Sign classification
                      - Inference
```

**Separation of Concerns:**
- Frontend: User interaction, display, audio
- Backend: Communication routing, orchestration
- ML Server: Computer vision, deep learning

---

## ğŸ“¦ Tech Stack

### Frontend
- **React 19** - UI framework
- **TypeScript** - Type safety
- **Socket.IO Client** - Real-time communication
- **Vite** - Build tool
- **Web Speech API** - Text-to-speech

### Backend
- **Node.js** - Runtime
- **Express** - Web framework
- **Socket.IO** - WebSocket server
- **ws** - WebSocket client (to ML)

### ML Server
- **Python 3.13+**
- **FastAPI** - Web framework
- **MediaPipe** - Hand landmark detection
- **PyTorch** - Deep learning inference
- **OpenCV** - Image processing
- **uvicorn** - ASGI server

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** 18+ and **pnpm**
- **Python** 3.13+
- **uv** (Python package manager) or pip
- A webcam

### 1ï¸âƒ£ Clone & Install

```bash
git clone <repository-url>
cd bridgio
```

### 2ï¸âƒ£ Start ML Server

```bash
cd ML
uv sync                    # Install dependencies
uv run python app.py       # Start server on port 8000
```

Or with pip:
```bash
cd ML
pip install -r requirements.txt
python app.py
```

### 3ï¸âƒ£ Start Backend

```bash
cd backend
pnpm install              # Install dependencies
pnpm dev                  # Start server on port 3000
```

### 4ï¸âƒ£ Start Frontend

```bash
cd frontend
pnpm install              # Install dependencies
pnpm dev                  # Start dev server on port 5173
```

### 5ï¸âƒ£ Open Browser

Navigate to **http://localhost:5173**

Allow camera access when prompted.

---

## ğŸ“š Project Structure

```
bridgio/
â”œâ”€â”€ frontend/              # React + TypeScript UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ CameraStream.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ OutputBox.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ HistoryPanel.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ StatusIndicator.tsx
â”‚   â”‚   â”‚   â””â”€â”€ ThemeToggle.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/         # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useTranslationHistory.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ useTextToSpeech.ts
â”‚   â”‚   â”‚   â””â”€â”€ useTheme.ts
â”‚   â”‚   â”œâ”€â”€ App.tsx        # Main app component
â”‚   â”‚   â”œâ”€â”€ socket.ts      # Socket.IO client
â”‚   â”‚   â””â”€â”€ types.ts       # TypeScript types
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/               # Node.js WebSocket bridge
â”‚   â”œâ”€â”€ server.js          # Express server
â”‚   â”œâ”€â”€ socket.js          # Frontend WebSocket handler
â”‚   â”œâ”€â”€ pythonClient.js    # ML server WebSocket client
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ ML/                    # Python ML server
    â”œâ”€â”€ app.py             # FastAPI application
    â”œâ”€â”€ classifier.py      # PyTorch inference
    â”œâ”€â”€ hand_tracking.py   # MediaPipe landmarks
    â”œâ”€â”€ model.py           # Neural network architecture
    â”œâ”€â”€ model.pt           # Trained weights
    â”œâ”€â”€ labels.json        # Sign labels
    â””â”€â”€ dataset/           # Training data
```

---

## ğŸ¨ UI Features

### Dark/Light Mode
Click the theme toggle (â˜€ï¸/ğŸŒ™) in the header to switch themes. Preference is saved to localStorage.

### Translation History
- Scrollable list of all recognized signs
- Timestamps for each entry
- ğŸ”Š indicator for spoken words
- Clear history button

### Connection Status
Live indicator shows:
- âœ… **Connected** - All systems operational
- âŒ **Disconnected** - Connection lost (auto-reconnect)

### Camera Feed
- Live video preview
- Recording indicator
- Error messages if camera access denied

---

## ğŸ”Œ API Reference

### Backend â†’ ML Server WebSocket

**Connection:** `ws://localhost:8000/ws`

**Send Frame:**
```json
{
  "frame": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
}
```

**Receive Prediction:**
```json
{
  "text": "HELLO"
}
```

### Frontend â†’ Backend WebSocket

**Connection:** `http://localhost:3000`

**Events:**
- `frame` (client â†’ server) - Send video frame
- `prediction` (server â†’ client) - Receive translation

---

## ğŸ§ª Supported Signs

The system currently recognizes 16 signs:

| Sign | Label |
|------|-------|
| ğŸ‘ | BUT |
| âœŠ | DO |
| ğŸ‘‡ | DOWN |
| â¤ï¸ | HEART |
| ğŸ†˜ | HELP |
| ğŸ¤Ÿ | I LOVE YOU |
| ğŸ˜˜ | KISS |
| ğŸ‘ˆ | ME |
| ğŸ‘Œ | OK |
| âœŒï¸ | OR |
| â˜®ï¸ | PEACE |
| ğŸ™ | PRAY |
| ğŸ¤ | SAME |
| ğŸ›‘ | STOP |
| ğŸ‘‰ | THAT |
| ğŸ‘† | UP |

---

## ğŸ”§ Configuration

### Frontend
Edit [frontend/src/socket.ts](frontend/src/socket.ts):
```typescript
const socket = io("http://localhost:3000")  // Backend URL
```

### Backend
Edit [backend/pythonClient.js](backend/pythonClient.js):
```javascript
new WebSocket("ws://localhost:8000/ws")  // ML server URL
```

Environment variables:
```bash
PORT=3000  # Backend port
```

### ML Server
Edit [ML/app.py](ML/app.py):
```python
uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## ğŸš€ Production Deployment

### 1ï¸âƒ£ Build Frontend
```bash
cd frontend
pnpm build
# Output: dist/
```

Serve with nginx, Vercel, or Netlify.

### 2ï¸âƒ£ Deploy Backend
```bash
cd backend
# Use PM2, Docker, or cloud service
pm2 start server.js --name sign-backend
```

### 3ï¸âƒ£ Deploy ML Server
```bash
cd ML
# Use Docker or cloud GPU service
uvicorn app:app --host 0.0.0.0 --port 8000
```

### Docker Example
```dockerfile
# ML Server
FROM python:3.13
WORKDIR /app
COPY ML/ .
RUN pip install -r requirements.txt
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ğŸŒ Ethical Considerations

### Accessibility First
- **Free for individuals** with disabilities
- No paywalls on core features
- WCAG 2.1 AA compliant design
- Keyboard navigation support

### Business Model
Revenue from:
- Educational institutions
- NGOs and government programs
- Corporate accessibility training
- Grants and donations

**Never charge disabled users.**

---

## ğŸ”® Future Enhancements

- [ ] **Dynamic signs** - LSTM for gesture sequences
- [ ] **Sentence formation** - Multi-word translation
- [ ] **Multilingual** - Support multiple sign languages
- [ ] **AR Glasses** - Real-time subtitles
- [ ] **Mobile apps** - iOS/Android deployment
- [ ] **Offline mode** - Local inference
- [ ] **Custom signs** - User-trained gestures
- [ ] **Analytics** - Usage insights (privacy-preserving)

---

## ğŸ› Troubleshooting

### Camera not working
- Check browser permissions (chrome://settings/content/camera)
- Ensure no other app is using the camera
- Try a different browser

### ML Server won't start
```bash
# Check if model file exists
ls ML/model.pt

# Verify Python dependencies
cd ML
uv run python -c "import torch, mediapipe; print('OK')"
```

### Backend connection fails
```bash
# Check if ML server is running
curl http://localhost:8000/health

# Check backend logs
cd backend
pnpm dev
```

### No predictions appearing
- Ensure good lighting
- Keep hands visible in frame
- Check browser console for errors
- Verify WebSocket connections in DevTools

---

## ğŸ“„ License

MIT License - Free for educational and accessibility use.

---

## ğŸ‘¥ Contributing

Contributions welcome! Focus areas:
- New sign language support
- Accessibility improvements
- Performance optimization
- Documentation

---

## ğŸ™ Acknowledgments

- **MediaPipe** - Hand tracking
- **PyTorch** - Deep learning
- **FastAPI** - ML API framework
- **Socket.IO** - Real-time communication

---

## ğŸ“ Support

For issues or questions:
1. Check documentation
2. Search existing issues
3. Create new issue with reproduction steps

**Built with â¤ï¸ for accessibility and inclusion.**
